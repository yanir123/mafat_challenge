{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mafat Sat Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Polygon\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import transforms as T\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import utils\n",
    "import engine\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.use('TKAgg', force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_class = {'small_vehicle': 'r', 'bus': 'g', 'medium_vehicle': 'b', 'large_vehicle': 'c', 'double_trailer_truck': 'm', 'container': 'y',\n",
    "               'pylon': 'k', 'small_aircraft': 'purple', 'large_aircraft': 'brown', 'small_vessel': 'orange', 'medium_vessel': 'pink', 'large_vessel': 'olive',\n",
    "               'heavy_equipment': 'tab:olive'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_image = '72_0_0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata_train.csv')\n",
    "metadata = metadata[metadata['Frame'] == curr_image]\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(rf'images\\{curr_image}.tiff')\n",
    "imarray = np.array(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'category_id']\n",
    "\n",
    "labels = pd.read_csv(\n",
    "    rf'labelTxt\\{curr_image}.txt', sep=\" \", header=None, names=columns_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "ax.imshow(imarray, cmap='gray')\n",
    "ax.grid(False)\n",
    "for _, label in labels.iterrows():\n",
    "    ax.add_patch(patches.Polygon(label[['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']].to_numpy(\n",
    "    ).reshape((4, 2)), linewidth=1, edgecolor=color_class[label['category_id']], facecolor='none'))\n",
    "ax.legend([plt.Line2D([0, 0], [0, 0], color=color, marker='o', linestyle='')\n",
    "          for color in color_class.values()], color_class.keys())\n",
    "plt.show()\n",
    "print(f'shape: {imarray.shape}, dtype: {imarray.dtype}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = 'images'\n",
    "label_dir_path = 'labelTxt'\n",
    "metadata_file_path = 'metadata_train.csv'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the metadata from the file.\n",
    "\n",
    "The metadata could potentially be used for kfolds corss-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_file_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the labels from the files.\n",
    "\n",
    "Note that the image id is not written in the file but can be derived from the files name and added to the dataframe as the field `Frame`\n",
    "\n",
    "The labels are a polygon inside the image boundires that has a `category_id` assined to it.\n",
    "\n",
    "`category_id` will be canged to an enum starting form 1 because 0 is saved for `background` and saved in `class_id`\n",
    "\n",
    "It can be inferred that pixels that are not assigned a value are `background`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_names = ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'category_id']\n",
    "labels = pd.DataFrame(columns=columns_names + ['Frame'])\n",
    "\n",
    "for label_file_name in os.listdir(label_dir_path):\n",
    "    curr_labels = pd.read_csv(\n",
    "        fr'{label_dir_path}\\{label_file_name}', sep=\" \", header=None, names=columns_names)\n",
    "    curr_labels['Frame'] = label_file_name.split('.')[0]\n",
    "    labels = pd.concat((labels, curr_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['class_id'], categories = pd.factorize(labels['category_id'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = labels.groupby('class_id')['class_id'].count()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(x=categories, y=class_distribution)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = labels.shape[0]\n",
    "confidence_level = 0.95\n",
    "margin_of_error = 0.05\n",
    "target_proportion = 0.5\n",
    "z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_scaled_sample_size = z_score ** 2 * target_proportion * \\\n",
    "    (1 - target_proportion) / margin_of_error ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = math.ceil(un_scaled_sample_size /\n",
    "                        (1 + (un_scaled_sample_size - 1) / population_size))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get balanced data samples\n",
    "\n",
    ">potential undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, train_labels, test_labels = train_test_split(\n",
    "    np.zeros(labels.shape[0]), labels, random_state=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = train_labels.groupby('class_id')['class_id'].count()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(x=categories, y=class_distribution)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = test_labels.groupby('class_id')['class_id'].count()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.barplot(x=categories, y=class_distribution)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images will be added via pytorch `torch.utils.data.Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MafatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transforms) -> None:\n",
    "        self.classes = {'small_vehicle': 1, 'bus': 2, 'medium_vehicle': 3, 'large_vehicle': 4, 'double_trailer_truck': 5, 'container': 6,\n",
    "                        'pylon': 7, 'small_aircraft': 8, 'large_aircraft': 9, 'small_vessel': 10, 'medium_vessel': 11, 'large_vessel': 12,\n",
    "                        'heavy_equipment': 13}\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.images_paths = np.array(os.listdir(images_dir))\n",
    "\n",
    "        self.masks_paths = np.array(os.listdir(masks_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.images_dir, self.images_paths[idx]))\n",
    "\n",
    "        columns_names = ['x1', 'y1', 'x2', 'y2',\n",
    "                         'x3', 'y3', 'x4', 'y4', 'category_id']\n",
    "        mask = pd.read_csv(os.path.join(\n",
    "            self.masks_dir, self.masks_paths[idx]), sep=' ', header=None, names=columns_names)\n",
    "\n",
    "        maxx = mask.filter(regex='x\\d').max(axis=1)\n",
    "        minx = mask.filter(regex='x\\d').min(axis=1)\n",
    "        maxy = mask.filter(regex='y\\d').max(axis=1)\n",
    "        miny = mask.filter(regex='y\\d').min(axis=1)\n",
    "\n",
    "        boxes = torch.as_tensor(\n",
    "            np.c_[minx, miny, maxx, maxy], dtype=torch.float32)\n",
    "\n",
    "        labels = torch.as_tensor(mask['category_id'].map(\n",
    "            self.classes), dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd,\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images_paths.shape[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a fastRcnn model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the models last layer to finetune it to this datasets classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "num_classes = 14  # plus one for the background\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "    in_features, num_classes=num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer function from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.UInt16PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate over dataset to see what the model expects during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatset = MafatDataset(image_dir_path, label_dir_path, get_transform(True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.utils.data.Subset(datatset, np.arange(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    sample, collate_fn=utils.collate_fn, batch_size=5, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(image for image in images)\n",
    "# targets = [{k: v[i] for k, v in targets.items()} for i in range(targets['boxes'].shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(images, targets)\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model on the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dataset and defined transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MafatDataset(images_dir=image_dir_path,\n",
    "                       masks_dir=label_dir_path, transforms=get_transform(True))\n",
    "dataset_test = MafatDataset(images_dir=image_dir_path,\n",
    "                            masks_dir=label_dir_path, transforms=get_transform(True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(dataset)\n",
    "test_size = math.ceil(data_size * 0.2)\n",
    "\n",
    "indices = torch.randperm(data_size).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-test_size])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-test_size:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model to right device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params=params, lr=5e-3,\n",
    "                            momentum=0.9, weight_decay=5e-4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    engine.train_one_epoch(model, optimizer, data_loader,\n",
    "                           device, epoch, print_freq=100)\n",
    "    lr_scheduler.step()\n",
    "    engine.evaluate(model, data_loader_test, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = dataset_test[0]\n",
    "\n",
    "images = images.to(device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.forward([images], [targets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'small_vehicle': 1, 'bus': 2, 'medium_vehicle': 3, 'large_vehicle': 4, 'double_trailer_truck': 5, 'container': 6,\n",
    "           'pylon': 7, 'small_aircraft': 8, 'large_aircraft': 9, 'small_vessel': 10, 'medium_vessel': 11, 'large_vessel': 12,\n",
    "           'heavy_equipment': 13}\n",
    "\n",
    "classes = {v: k for k, v in classes.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_labels = res[0]['labels'].to(device='cpu').numpy()\n",
    "res_boxes = res[0]['boxes'].to(device='cpu').detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "ax.imshow(images.to(device='cpu').reshape((1280, 1280)), cmap='gray')\n",
    "ax.grid(False)\n",
    "\n",
    "index = 0\n",
    "for bbox in res_boxes:\n",
    "    ax.add_patch(patches.Rectangle(bbox[0:2], bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                 linewidth=1, edgecolor=color_class[classes[res_labels[index]]], facecolor='none'))\n",
    "\n",
    "    index += 1\n",
    "\n",
    "ax.legend([plt.Line2D([0, 0], [0, 0], color=color, marker='o', linestyle='')\n",
    "          for color in color_class.values()], color_class.keys())\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b55e0ebdea1152012b3e638cc75f4eb19f0ef1dc7f1ff8ef3b235b0f35649df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
